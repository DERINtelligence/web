<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-09-17T00:14:57+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Derintelligence</title><subtitle>A research project, supports AI in Azerbaijan</subtitle><entry><title type="html">Guess who’s back’propagate. - Zəncir qaydası və geriyə yayılma.</title><link href="http://localhost:4000/suni_neyron_shebekeler_geriye_yayilma/" rel="alternate" type="text/html" title="Guess who's back'propagate. - Zəncir qaydası və geriyə yayılma." /><published>2019-02-21T00:00:00+03:00</published><updated>2019-02-21T00:00:00+03:00</updated><id>http://localhost:4000/suni_neyron_shebekeler_geriye_yayilma</id><content type="html" xml:base="http://localhost:4000/suni_neyron_shebekeler_geriye_yayilma/">&lt;p&gt;Salamlar, keçən &lt;a href=&quot;http://derintelligence.az/suni_neyron_shebekeler_mashin_oyrenme/&quot;&gt;yazıda&lt;/a&gt; maşın öyrənmənin təməlini təşkil edən xəta funksiyalarından və ən önəmli alqoritmlərdən olan nöqtəvi meyilli azalma haqqında danışmışdıq. Bu dəfə istəyirəm, keçən dəfə qeyd etdiklərimin süni neyron şəbəkələrdə necə istifadə edildiyi haqqında danışım. Belə ki, bu yazı nöqtəvi meyilli azalmanın neyron şəbəkədə tətbiqi olan geriyə yayılma və onun həlli üçün istifadə edəcəyimiz riyazi metod olan zəncir qaydası ilə bağlıdır.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/DERINtelligence/web/master/images/lausanne.jpg&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Gəlin ilk olaraq neyron şəbəkə arxitekturasını və irəliyə ötürmə qaydasını yada salaq.&lt;/p&gt;
&lt;div class=&quot;center&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/DERINtelligence/web/master/images/neuralnetwork.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Qeyd: Bu yazıda xəta funksiyası (&lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;), və son qat (&lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;) qarışmasın deyə, xəta funksiyasını &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; ilə əvəz edəcəm.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
        z^{(l)} = w^{(l)} x^{(l-1)} + b^{(l)}
\tag{1}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
        x^{(l)} = \sigma(z^{(l)})
\tag{2}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;İndi isə xəta funksiyası və stoxastik nöqtəvi meyilli azalmanı xatırlayaq.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
		C = \frac{1}{2}{(Y - f)} ^ 2
\tag{3}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;Süni neyron şəbəkədə riyazi modelin çıxış dəyəri şəbəkənin çıxış dəyərinə bərabərdir. Ona görə də Bər. 3-dəki &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; funksiyası &lt;script type=&quot;math/tex&quot;&gt;x_{(L)}&lt;/script&gt; - ə bərabər olacaqdır. Bu vəziyyətdə Bər. 3-ü belə yaza bilərik:&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
		C = \frac{1}{2}{(Y - x^{(L)})} ^ 2
\tag{4}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;Bu xəta funksiyasına uyğun nöqtəvi meyilli azalma isə ağırlıq və sürüşmə əmsalının hər bir iterasiyada dəyişməsindən ibarətdir.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
		w^{(t+1)} = w^{(t)} - \beta_1\frac{\partial C}{\partial w^{(t)}}
\tag{5}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
		b^{(t+1)} = b^{(t)} - \beta_2\frac{\partial }{\partial b^{(t)}}
\tag{6}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;Bər. 4-də gördüyümüz kimi biz xəta dəyərini hesablayarkən şəbəkənin çıxış dəyəri olan &lt;script type=&quot;math/tex&quot;&gt;x^{(L)}&lt;/script&gt; - in verilən toplusundakı cavab ilə kvadratik fərqini hesablayırıq. &lt;script type=&quot;math/tex&quot;&gt;x^{(L)}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;z^{(L)}&lt;/script&gt;-dən(Bər 2.), &lt;script type=&quot;math/tex&quot;&gt;z_{(L)}&lt;/script&gt; isə öz növbəsində ağırlıq və sürüşmə dəyərlərindən(Bər 1.) asılıdır. Yəni &lt;script type=&quot;math/tex&quot;&gt;x^{(L)}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;w^{(L)}&lt;/script&gt; və &lt;script type=&quot;math/tex&quot;&gt;b^{(L)}&lt;/script&gt;-dən asılıdır. Ancaq burda önəmli bir sual yaranır, xətanı azaltmaq üçün Bər 5. və Bər 6. yalnızca &lt;script type=&quot;math/tex&quot;&gt;w^{(L)}&lt;/script&gt; və &lt;script type=&quot;math/tex&quot;&gt;b^{(L)}&lt;/script&gt; - ə tətbiq etməliyik?&lt;/p&gt;

&lt;p&gt;Belə olduğu halda şəbəkənin gizli qatlarına uyğun ağırlıq və sürüşmə əmsallarını yox saymış olacağıq. Ancaq Bər 1.-də də gördüyümüz kimi hər qatın dəyəri özündən əvvəlki qatın dəyərindən də asılıdır, yəni ağırlıqlarda və sürüşmədə uyğun dəyişiklik edilməsə, çıxış qatının dəyəri tam optimal olmayacaq. Problem ondadır ki, biz xəta dəyərinin yalnızca çıxış qatında hesablaya bilirik. Bəs onda biz gizli qatlardakı parametrlərində nəyə uyğun dəyişliklər edəcəyik? Başqa bir sözlə desək, nöqtəvi meyilli azalmanı gizli qatları necə tətbiq etmək olar?&lt;/p&gt;

&lt;p&gt;Şəbəkədə hər qat özündən bir əvvəlki qatdan asılı olduğundan, çıxış qatınının dəyərini təyin oblastı giriş qatı olan mürəkkəb qeyri-xətti funksiya hesab edə bilərik. Yəni şəbəkənin əsas irəliyə ötürmə prinsipini açılmış şəkildə belə yaza bilərik.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
   z^{(1)} &amp;= w^{(1)} x^{(0)} + b^{(1)} \\
   x^{(1)} &amp;= \sigma(z^{(1)}) \\
   z^{(2)} &amp;= w^{(2)} x^{(1)} + b^{(2)} \\
   x^{(2)} &amp;= \sigma(z^{(2)}) \\
   &amp; \dots \\
   z^{(l)} &amp;= w^{(l)} x^{(l-1)} + b^{(l)} \\
   x^{(l)} &amp;= \sigma(z^{(l)}) \\
   &amp; \dots \\
   z^{(L)} &amp;= w^{(L)} x^{(L-1)} + b^{(L)} \\
   x^{(L)} &amp;= \sigma(z^{(L)}) \\
\end{split}
\tag{7}\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Çıxış dəyərinin mürəkkəb funksiya olması bizə hər bir qat üçün nöqtəvi meyili hesablayarkən çox kömək olacaq. Belə ki, biz differensialları hesablayarkən mürəkkəb funksiyaya tətbiq olunan ən önəmli qaydalardan olan &lt;strong&gt;zəncir qaydasından&lt;/strong&gt; istifadə edəcəyik. Bu qayda ilə, yəqin ki, oxuyucularımızın əksəriyyəti tanışdır, ancaq, gəlin, qısaca xatırlayaq.&lt;/p&gt;

&lt;h1 id=&quot;zəncir-qaydası&quot;&gt;Zəncir Qaydası&lt;/h1&gt;

&lt;p&gt;İxtiyari &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; və &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; funksiyasıları üçün, &lt;script type=&quot;math/tex&quot;&gt;F=f(g(x))&lt;/script&gt; qaydasını qəbul edək. Məqsədimiz &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt;-in &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; parametrinə uyğun törəməsini hesablamaqdır. Gəlin,&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;g(x)&lt;/script&gt; - i &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;f(y)&lt;/script&gt; - i isə &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; ilə işarə edək. Bu halda zəncir qaydası aşağıdakı kimidir:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
   F^\prime &amp;= \frac{dz}{dx} \\
   \frac{dz}{dx} &amp;= \frac{dz}{dy} \frac{dy}{dx} \\
   \frac{dz}{dx} &amp;= f^\prime(g(x)) g^\prime(x) \\
   F^\prime &amp;= \frac{dz}{dx} =  f^\prime(g(x)) g^\prime(x)\\
\end{split}
\tag{8}\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;İndi isə qayıdaq əsas problemə - nöqtəvi meyillərin hesablanmasına. Bizim məqsədimiz hər bir qat üçün nöqtəvi meyillər - &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial C}{\partial w^{(t)}}&lt;/script&gt; və &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial C}{\partial b^{(t)}}&lt;/script&gt; - i hesablamaqdır. İlk addım olaraq bu hesablamaları asanlaşdırmaq üçün əlavə bir parametrdən - &lt;script type=&quot;math/tex&quot;&gt;\delta&lt;/script&gt; -dan istifadə edəcəyik.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
  \delta^l = \frac{\partial C}{\partial z^l}, \forall l \in [1, L]
\tag{9}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;Yuxarıda vurğuladığım kimi nöqtəvi meyil üçün gərəkli olan xəta funskiyasının dəyəri çıxış qatının dəyərindən asılıdır. Buna görə də, ilk olaraq bu qata uyğun meylin hesablamasından başlayırıq. Mən buradakı hesablamalar zamanı nəticələrin həll yolunu da izah edəcəyəm, əgər “riyaziyyatı boş ver, mənə cavabı ver” deyirsinizsə, hər bərabərliyin son nəticəsinə baxmağınız kifayətdir.&lt;/p&gt;

&lt;p&gt;İlk olaraq çıxış qatında başlayırıq.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
  \delta^L &amp;amp;= \frac{\partial C}{\partial z^L_j} &lt;br /&gt;
\tag{9}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;Növbəti addım kimi bu bərabərliyə zəncir qaydasını tətbiq edirik.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
  \delta^L &amp;amp;= \frac{\partial C}{\partial x^L} \frac{\partial x^L}{\partial z^L}
\tag{11}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;x^L = \sigma(z^L)&lt;/script&gt; bərabərliyini xatırlasaq, yuxarıdakı bərabərliyin ikinci hissəsini &lt;script type=&quot;math/tex&quot;&gt;\sigma'(z^L_j)&lt;/script&gt; kimi yaza bilərik.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
  \delta^L = \frac{\partial C}{\partial x^L} \sigma’(z^L_j)
\tag{12}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;Diqqət etsək, görə bilərik ki, bərabərliyin birinci hissəsi isə Bər 4. - ün differensialıdır. Buna görə ifadəni belə yaza bilərik.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray} 
  \delta^L = (x^L-y) \sigma’(z^L).
\tag{13}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\delta^L&lt;/script&gt; şəbəkənin &lt;strong&gt;çıxış itkisi&lt;/strong&gt;(ing. output error) adlanır. Növbəti addımlarda məqsədimiz bu itkinin gizli qatlara yayılmasını təmin etməkdir. Ümumi ideya bundan ibarətdir ki, hər bir qatdakı itkini ondan əvvəlki qatdakı itkini hesablamaq üçün istifadə edəcəyik. Bu rekursiv məntiq səbəbi ilə istifadə etdiyimiz bu alqoritm itkinin geriyə yayılması adlanır. Hər qatda hesablayacağımız bu itkinin köməkliyi ilə ümumi xətanın hər qatdakı ağırlıq və sürüşməyə nəzərən olan meyilli azalmasını tapa biləcəyik. Beləliklə, bu bizə hər qatdakı ağırlıq və sürüşməni yeniləməyə və optimal həlli tapmağa imkan verəcək.&lt;/p&gt;

&lt;p&gt;Növbəti addımda isə ixtiyari gizli qat &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; üçün itki - &lt;script type=&quot;math/tex&quot;&gt;\delta^l&lt;/script&gt; - in hesablanmasına baxaq.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
  \delta^l &amp;amp;= \frac{\partial C}{\partial z^l_j} &lt;br /&gt;
\tag{14}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;Çıxış qatından fərqli olaraq gizli qat bir neyron yox, bir neçə neyrondan ibarət ola bilər. &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; qatındakı hər bir &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; neyronu tam əlaqələnmiş(ing. fully connected) neyron şəbəkədə &lt;script type=&quot;math/tex&quot;&gt;l+1&lt;/script&gt; qatındakı bütün neyronlarla bağlanmışdır. Buna görə də Bər. 13-ü zəncir qaydasından istifadə edərək, hər bir &lt;script type=&quot;math/tex&quot;&gt;\delta^l_j&lt;/script&gt; üçün belə yaza bilərik.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{eqnarray}
  \begin{split}
    \delta^l_j &amp;= \frac{\partial C}{\partial z^l_j} \\
    &amp;= \sum_k \frac{\partial C}{\partial z^{l+1}_k} \frac{\partial z^{l+1}_k}{\partial z^l_j} \\
    &amp;= \sum_k \delta^{l+1}_k \frac{\partial z^{l+1}_k}{\partial z^l_j}
  \end{split}
\tag{15}\end{eqnarray} %]]&gt;&lt;/script&gt;

&lt;p&gt;İfadənin ikinci hissəsini hesablamaq üçün &lt;script type=&quot;math/tex&quot;&gt;l+1&lt;/script&gt; qatı üçün irəliyə ötürmə qaydasından istifadə edə bilərik.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{eqnarray}
  z^{l+1}_k = \sum_j w^{l+1}_{kj} x^l_j +b^{l+1}_k = \sum_j w^{l+1}_{kj} \sigma(z^l_j) +b^{l+1}_k.
\tag{16}\end{eqnarray}&lt;/script&gt;

&lt;p&gt;Buna uyğun differensialı həll edə bilərik:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{eqnarray}
  \frac{\partial z^{l+1}_k}{\partial z^l_j} = w^{l+1}_{kj} \sigma'(z^l_j).
\tag{17}\end{eqnarray}&lt;/script&gt;

&lt;p&gt;Hər bir şeyi birləşdirdikdə, Bər 13-ü belə yaza bilərik&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{eqnarray}
  \delta^l_j = \sum_k w^{l+1}_{kj}  \delta^{l+1}_k \sigma'(z^l_j).
\tag{18.1}\end{eqnarray}&lt;/script&gt;

&lt;p&gt;Bu bərabərliyi vektor formasında belə də yaza bilərik&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{eqnarray} 
  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l),
\tag{18.2}\end{eqnarray}&lt;/script&gt;

&lt;p&gt;Burada &lt;script type=&quot;math/tex&quot;&gt;\odot&lt;/script&gt; elementvari və ya &lt;a href=&quot;https://en.wikipedia.org/wiki/Hadamard_product_(matrices)&quot;&gt;Hadamart&lt;/a&gt; hasilini ifadə edir.&lt;/p&gt;

&lt;p&gt;Artıq &lt;script type=&quot;math/tex&quot;&gt;\delta^l, l = 1, 2, \dots, L&lt;/script&gt; bildiyimiz üçün onlardan istifadə edərək &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial C}{\partial w}&lt;/script&gt; və &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial C}{\partial b}&lt;/script&gt; həll edə bilərik.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{eqnarray}
  \frac{\partial C}{\partial w^l_{jk}} &amp;= \frac{\partial C}{\partial z^l_{j}}\frac{\partial z^l_{j}}{\partial w^l_{jk}} \\
  &amp;= \delta^l_j x^{l-1}_k
\tag{19.1}\end{eqnarray} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{eqnarray}
  \frac{\partial C}{\partial b^l_{j}} &amp;= \frac{\partial C}{\partial z^l_{j}}\frac{\partial z^l_{j}}{\partial b^l_{j}} \\
  &amp;= \delta^l_j
\tag{19.2}\end{eqnarray} %]]&gt;&lt;/script&gt;

&lt;p&gt;Qeyd: Arada bəzi sadə törəmə əməliyyatlarını sizin incələməyiniz üçün qəsdən buraxdım.&lt;/p&gt;

&lt;p&gt;Yekun olaraq bütün geriyə yayılma alqoritminin elementlərini birlikdə yazaq:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;İrəliyə ötürmə: Hər bir &lt;script type=&quot;math/tex&quot;&gt;l = 2, 3, \ldots, L&lt;/script&gt; üçün &lt;script type=&quot;math/tex&quot;&gt;z^l&lt;/script&gt; və &lt;script type=&quot;math/tex&quot;&gt;x^l&lt;/script&gt; - i hesablamaq.&lt;/li&gt;
  &lt;li&gt;Çıxış itkisi: &lt;script type=&quot;math/tex&quot;&gt;\delta^L&lt;/script&gt; - i hesablamaq.&lt;/li&gt;
  &lt;li&gt;İtkini geriyə yayma: Hər bir &lt;script type=&quot;math/tex&quot;&gt;l = L-1, L-2, \ldots, 2&lt;/script&gt; üçün &lt;script type=&quot;math/tex&quot;&gt;\delta&lt;/script&gt; - i hesablamaq&lt;/li&gt;
  &lt;li&gt;Nöqtəvi meyilləri hesablama: Xəta funksiyasının ağırlıq və sürüşməyə əsasən nöqtəvi meyillərini &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial C}{\partial w}&lt;/script&gt; və &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial C}{\partial b}&lt;/script&gt;  hesablamaq.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Mammad Hajili</name></author><summary type="html">Salamlar, keçən yazıda maşın öyrənmənin təməlini təşkil edən xəta funksiyalarından və ən önəmli alqoritmlərdən olan nöqtəvi meyilli azalma haqqında danışmışdıq. Bu dəfə istəyirəm, keçən dəfə qeyd etdiklərimin süni neyron şəbəkələrdə necə istifadə edildiyi haqqında danışım. Belə ki, bu yazı nöqtəvi meyilli azalmanın neyron şəbəkədə tətbiqi olan geriyə yayılma və onun həlli üçün istifadə edəcəyimiz riyazi metod olan zəncir qaydası ilə bağlıdır.</summary></entry><entry><title type="html">Guess who’s back’propagate. - Xəta funksiyası və stoxastik nöqtəvi meyilli azalma.</title><link href="http://localhost:4000/suni_neyron_shebekeler_mashin_oyrenme/" rel="alternate" type="text/html" title="Guess who's back'propagate. - Xəta funksiyası və stoxastik nöqtəvi meyilli azalma." /><published>2019-02-18T00:00:00+03:00</published><updated>2019-02-18T00:00:00+03:00</updated><id>http://localhost:4000/suni_neyron_shebekeler_mashin_oyrenme</id><content type="html" xml:base="http://localhost:4000/suni_neyron_shebekeler_mashin_oyrenme/">&lt;p&gt;Salamlar, keçən &lt;a href=&quot;http://derintelligence.az/suni_neyron_shebekeler_ireli_qidalama/&quot;&gt;yazıda&lt;/a&gt; süni neyron şəbəkənin arxitekturası haqqında danışdıq. Orada göstərdiyim “irəli qidalama” tərcüməsi ilə bağlı bir neçə mesaj aldım və biraz qəribə tərcümə olduğunu nəzərə alaraq onu “irəliyə ötürmə” kimi dəyişməyin daha düzgün olduğunu düşünürəm. Bundan əvvəlki yazıda da uyğun dəyişiklikləri edəcəyəm.&lt;/p&gt;

&lt;p&gt;Bu dəfəki yazımızda maşın öyrənmənin ümumi izahından, istifadə olunan bərabərliklərdən və alqoritmlərdən - xəta funksiyasından və stoxastik nöqtəvi meyilli azalma haqqında danışacam. Bu yazını yazarkən, oxuyucumun ən azı maşın öyrənmənin nə olduğunu bildiyini ümid edirəm. Hazır olun, bu yazıda bol-bol riyazi ifadələr görəcəksiniz. Kəmərləri bağlayın, başlayırıq.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/DERINtelligence/web/master/images/paris_blog.jpg&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Əslində bu yazını yazmaq bir qədər çətindir, çünki, maşın öyrənmə alqoritmlərini tam izah etmək üçün 1 yox, 5 yazı bəs etməz, böyük ehtimalla. Ona görə də mən bu yazıda yalnızca bizim SNŞ-dəki hesablamalar üçün lazım olan önəmli nüanslara toxunacam. Belə ki, maşın öyrənmə alqoritmləri(biz burada öyrənməni müşahidəli(ing. supervised) olaraq hesab edirik) verilənlər toplusu &lt;script type=&quot;math/tex&quot;&gt;X={X_1, X_2, \cdots, X_N}&lt;/script&gt; və ona uyğun cavablar &lt;script type=&quot;math/tex&quot;&gt;Y={Y_1, Y_2, \cdots, Y_N}&lt;/script&gt; əsasında qeyri-xətti və ya xətti bir qanunauyğunluq taparaq verilənlər toplusu ilə eyni quruluşda olan yeni verilənə uyğun cavabı təxmin etmək üçün istifadə edilir. Bu iki ardıcıl prosesə “öyrən və təxmin et” (ing. learn and predict) də deyə bilərik. Ancaq bu prosesdə, məncə bir nüans əskikdi, sizcə, hansı?&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; ölçülü bu topluda ən optimistik halda biz hər bir elementin yalnızca uyğun cavab dəyəri ilə uyğunluğunu görə bilərik. Ancaq toplunun hər verilənin uyğunluğu individual olaraq bizim məsələdə bir məna kəsb etmir, çünki ixtiyari iki verilən - &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; və &lt;script type=&quot;math/tex&quot;&gt;X_j&lt;/script&gt; fərqli-fərqli funksiyanın təyin oblastı ola bilər. Bizə ümumi qanunauyğunluğu tapmaq üçün bütün bu funksiyaları ehtiva edəcək ümumi funksiyanı - riyazi modeli tapmaq gərəklidir. Gəlin, bu funksiyanı &lt;script type=&quot;math/tex&quot;&gt;f(X)&lt;/script&gt; ilə işarə edək. Ən sadə halda bu funksiya arqumentləri girişə uyğun ağırlıq vektoru və sürüşmə əmsalı olan xətti funksiya ola bilər. Biz də hesablamalar zamanı izahı daha asan olsun deyə bu funksiyadan istifadə edəcəyik.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray} 
		f(X_i) = w \cdot X_i + b
\tag{1}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;Yuxarıda qeyd etdiyim verilənlər toplusunda gördüyünüz kimi hər bir element &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt;-in ona uyğun cavabı &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt; var. Deməli biz ümumi funksiyamızın təxmininin dəqiqliyini funksiyanın dəyərinin və verilən elementə uyğun cavabın bir-birinə nə qədər yaxın olduğu ilə müqayisə edə bilərik. Bu yaxınlığı/uzaqlığı hesablamaq üçün &lt;strong&gt;xəta funksiyasından&lt;/strong&gt;(ing. loss/cost function) istifadə edirik. Bu məsələmizdə xəta funksiyası kimi kvadratik xəta funksiyasında istifadə edəcəyik. &lt;script type=&quot;math/tex&quot;&gt;f(X)&lt;/script&gt; funksiyasının ümumi xətası bütün elementlərə uyğun xətaların cəmi olduğundan kvadratik funksiya cəm zamanı mənfi və müsbət xətaların bir-birini ixtisar etməsinin qarşısını alır. Gəlin &lt;script type=&quot;math/tex&quot;&gt;f(X)&lt;/script&gt; funksiyasının xəta funksiyasını &lt;script type=&quot;math/tex&quot;&gt;L(X)&lt;/script&gt; ilə işarə edək. Cəmi normallaşdırma üçün xətalar cəmini nöqtələrin sayına bölürük.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray} 
		L(X_i) = \frac{1}{2}{(Y_i - f(X_i))} ^ 2
\tag{2}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;\begin{eqnarray} 
		L(X) = \frac{1}{N}\sum_i L(X_i)
\tag{3}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Qeyd:&lt;/strong&gt; Kvadratik funksiyanı &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{2}&lt;/script&gt; əmsalı ilə vurmağımızın səbəbi sonrakı hesablamalar zamanı törəmə aldığımızda qalıq əmsalsız nəticə əldə etməkdir. Bu ümumi qəbul edilmiş bir qaydadır, ancaq, təbii ki, bütün hesablamaları bu əmsalı nəzərə almadan da etsək, yenə də eyni nəticəyə gələcəyik.&lt;/p&gt;

&lt;p&gt;Xəta funksiyası maşın öyrənmə alqoritmlərinin təməlini təşkil edir, belə ki, yuxarıda qeyd etdiyimiz əskik nüansı onu istifadə edərək tamamlaya bilərik. Bər. 1-də gördüyümüz kimi funksiyanın dəyəri ağırlıq və sürüşmə əmsalından asılıdır və Bər. 2-də gördüyümüz kimi xətanın dəyəri &lt;script type=&quot;math/tex&quot;&gt;f(X)&lt;/script&gt;-in dəyərindən asılıdır. Yəni, bilavasitə, modelin xətası &lt;script type=&quot;math/tex&quot;&gt;f(X)&lt;/script&gt; funksiyasının arqumentləri olan ağırlıq &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; və sürüşmə &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; dəyərlərindən asılıdır. Bizim məqsədimiz isə, ən dəqiq, yəni verilən cavaba ən yaxın dəyərlər təxmin etməkdir, başqa bir sözlə desək xətanı azaltmaqdır. Deməli, biz funksiyanın arqumentlərini dəyişməklə ən az xətalı modeli əldə edə bilərik. Yəni, ixtiyari təyin olunmuş &lt;script type=&quot;math/tex&quot;&gt;w^{(0)}&lt;/script&gt; və &lt;script type=&quot;math/tex&quot;&gt;b^{(0)}&lt;/script&gt;-dən başlayaraq, xəta azalana qədər onları &lt;em&gt;müəyyən üsulla&lt;/em&gt; dəyişsək modelimiz öyrənmə prosesini bitirəcək. Ona görə də, mən təklif edirəm ki, bu iterativ məntiqi də nəzərə alaraq, gəlin, “öyrən və təxmin et” prosesini “öyrədilənə qədər davam et və təxmin et” olaraq dəyişək və əskik nüansı aradan qaldıraq.&lt;/p&gt;

&lt;p&gt;Yuxarıdakı abzasda sondan ikinci cümlədə fikir versəniz “müəyyən üsulla” hissəsini xüsusi işarələdim. İndi bizi maraqlandıran isə bizə ən dəqiq modeli verəcək ən optimal &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; və &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;-ni hər iterasiyada hansı üsulu istifadə edərək dəyişəcəyimizi müəyyən etməkdir. Maşın öyrənmədə, təbii ki, bir-birindən fərqli müxtəlif üsullar təklif olunub, ancaq mən sizə SNŞ-lərin döyünən ürəyi olan &lt;strong&gt;stoxastik nöqtəvi meyilli azalmanı(SNMA)&lt;/strong&gt;(ing. stochastic gradient descent(SGD)) göstərəcəm. Bu arada, bu mövzu hələ də üzərində kifayət qədər elmi araşdırma gedən mövzudur və dərin riyazi izahı vaxt alandır, ona görə də mən riyazi analizin axtarışını sizin ixtiyarınıza buraxıram. Bununla belə, bizə lazım olacaq riyazi izahı verəcəm, təbii ki.&lt;/p&gt;

&lt;div class=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/DERINtelligence/web/master/images/sgd.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Belə ki, &lt;strong&gt;nöqtəvi meyil(ing. gradient)&lt;/strong&gt; funksiyanın toxunanının müəyyən nöqtədəki meylidir. Nöqtəvi meyil, xəta funksiyasının artış istiqamətinin əksinə olan vektor olaraq da ifadə edilir. Ancaq biz burda qarışıqlıq olmasın deyə onun ədədi dəyəri ilə ifadə edəcəyik. Belə ki, ədədi dəyər ilə ifadə edildiyində nöqtəvi meyil, sadəcə xəta funksiyasının müəyyən nöqtədəki differensialının əksinə bərabər olur. &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; funksiyası &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; və &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;-dən asılı funksiya olduğundan, iki nöqtəvi meyildən istifadə edəcəyik. Belə ki, hər hansı bir arqumentin &lt;script type=&quot;math/tex&quot;&gt;t+1&lt;/script&gt; anındakı dəyərini tapmaq üçün onun &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; anındakı dəyəri ilə həmin andakı nöqtəvi meylini toplayacağıq.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray} 
		w^{(t+1)} = w^{(t)} - \beta_1\frac{\partial L_n}{\partial w^{(t)}}
\tag{4}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;\begin{eqnarray} 
		b^{(t+1)} = b^{(t)} - \beta_2\frac{\partial L_n}{\partial b^{(t)}}
\tag{5}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;Burada &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt; və &lt;script type=&quot;math/tex&quot;&gt;\beta_2&lt;/script&gt; xəta dəyərinin çox böyük qiymətlərində &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; və &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;-nin kəskin şəkildə azalmasının qarşısını alaraq modelin normallaşdırılması üçün istifadə olunan əmsallardır. &lt;script type=&quot;math/tex&quot;&gt;L_n&lt;/script&gt; isə verilənlər toplusundan ixtiyari seçilmiş &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; elementinin xəta dəyəridir. Bu element hər iterasiyada ixtiyari olaraq seçildiyindən bu alqoritmi &lt;strong&gt;stoxastik&lt;/strong&gt; nöqtəvi meyilli azalma adlandırırıq.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Əlavə tapşırıq:&lt;/strong&gt; Bər. 2-də &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; funksiyasını görürük. Bu bərabərlik əsasında törəmələri həll edərək Bər 4. və Bər 5.-i yenidən göstərin.&lt;/p&gt;

&lt;p&gt;Bir daha xatırlatmaq istəyirəm ki, &lt;script type=&quot;math/tex&quot;&gt;f(X)&lt;/script&gt; funksiyasını izahı sadə olduğundan istifadə etdim, növbəti yazıda indi öyrəndiyimiz alqoritmlərin SNŞ-lərə tətbiqi zamanı &lt;script type=&quot;math/tex&quot;&gt;f(X)&lt;/script&gt; yox, gizli qatların aktivasiyası və yekun çıxış qatının dəyərindən istifadə edəcəyik. Açığı, burada xəta funksiyası və SNMA-nı yaxşı anlasanız, növbəti yazı sizin üçün yalnızca mürəkkəb funksiyaların törəməsinin &lt;strong&gt;zəncir qaydası&lt;/strong&gt; ilə həllini başa düşməkdən ibarət olacaq. Bəli, gələn yazıda seriyanın adını daşıyan geriyə yayılma(backpropagation) alqoritmindən danışacağıq.&lt;/p&gt;

&lt;p&gt;Buraya qədər səbr edib oxuduğunuz üçün təşəkkürlər!&lt;/p&gt;</content><author><name>Mammad Hajili</name></author><summary type="html">Salamlar, keçən yazıda süni neyron şəbəkənin arxitekturası haqqında danışdıq. Orada göstərdiyim “irəli qidalama” tərcüməsi ilə bağlı bir neçə mesaj aldım və biraz qəribə tərcümə olduğunu nəzərə alaraq onu “irəliyə ötürmə” kimi dəyişməyin daha düzgün olduğunu düşünürəm. Bundan əvvəlki yazıda da uyğun dəyişiklikləri edəcəyəm.</summary></entry><entry><title type="html">Guess who’s back’propagate. - Süni neyron şəbəkə və irəliyə ötürmə alqoritmi.</title><link href="http://localhost:4000/suni_neyron_shebekeler_ireli_qidalama/" rel="alternate" type="text/html" title="Guess who's back'propagate. - Süni neyron şəbəkə və irəliyə ötürmə alqoritmi." /><published>2019-02-14T00:00:00+03:00</published><updated>2019-02-14T00:00:00+03:00</updated><id>http://localhost:4000/suni_neyron_shebekeler_ireli_qidalama</id><content type="html" xml:base="http://localhost:4000/suni_neyron_shebekeler_ireli_qidalama/">&lt;p&gt;Salamlar, aylar oldu bizdən yazı görmədiyiniz, bilirik. Deməli məsələ belə idi ki, bizim saytımız çökdü, problemlər oldu və biz daha sadə bir struktura keçməyə qərar verdik. Artıq yazılara Wordpress’dən yox, Jekyll-Now dizaynı olan sadə bir saytdan davam edəcəyik (bəlkə, ə’lər də xoşunuza gələr bu dəfə). Bu arada, neyronlar və siqmoid funksiya haqda olan iki yazı silinib, onları yenidən bərpa etməyə çalışacam.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/DERINtelligence/web/master/images/london_blog.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Deməli, keçək mətləbə. Bugün süni neyron şəbəkənin strukturuna giriş edəcəyik. Əgər ümumi neyron anlayışına və aktivasiya funksiyaları haqda ümumi anlayışınız varsa, SNŞ-lərin sadə quruluşu sizin üçün çətin olmayacaq. (bundan sonra süni neyron şəbəkələri SNŞ’lə ifadə edəcəyik). Belə ki, mən bu yazıda verəcəyim arxitekturada aktivasiya funksiyasını siqmoid funksiya olaraq qəbul edirəm. Gəlin sadə neyronun siqmoidlə aktivasiyanı xatırlayaq.&lt;/p&gt;

&lt;div class=&quot;center&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/DERINtelligence/web/master/images/sigma.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;\begin{eqnarray} 
		z = w \cdot x + b
\tag{1}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
        \sigma(z) = \frac{1}{1+e^{-z}}
\tag{2}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;Bər. (1)’də &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; siqmoid neyronun giriş dəyərlərini, &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; girişə uyğun ağırlıqları, &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; isə sürüşmə dəyərini ifadə edir. Bər. (2) isə siqmoid aktivasiya funksiyasıdır və onunla neyronun çıxış dəyərini hesablayırıq.&lt;/p&gt;

&lt;p&gt;Bizim əsas məqsədimiz bu və ya oxşar(digər aktivasiya funksiyaları istifadə edilən) dizaynda olan neyronlardan birlikdə bir şəbəkə qurmaqdır və məsələ şəbəkədirsə, hətta bəzi hallarda bir neçə qat neyronlar toplusu istifadə etmək lazım ola bilər. Belə ki, bioloji neyron şəbəkənin strukturu əsasında süni neyronların bir-birilərinə riyazi və məntiqi qanunauyğunluqla əlaqələnərək əmələ gətirdiyi riyazi model &lt;strong&gt;süni neyron şəbəkə&lt;/strong&gt; adlanır.&lt;/p&gt;

&lt;p&gt;Ən sadə SNŞ-ə eyni giriş dəyərləri olan bir neçə perseptronu misal gətirə bilərik. Bu şəbəkənin girişi eynən perseptronda olduğu kimi &lt;script type=&quot;math/tex&quot;&gt;x_1, x_2,\dots, x_n&lt;/script&gt;, çıxışı isə yalnız bir dəyər yox, bir neçə perseptronun yekun dəyərləridir - &lt;script type=&quot;math/tex&quot;&gt;y_1, y_2, \dots, y_m&lt;/script&gt;. Belə ki, daha mürəkkəb modellər elə bu sadə modelin üzərində qurulur və əksər hallarda elə bu mürəkkəb modellərdən istifadə olunur.&lt;/p&gt;

&lt;p&gt;Şəbəkədə hər bir hesablama mərhələsi &lt;em&gt;qat&lt;/em&gt;(ingilis dilli ədəbiyyatlarda &lt;em&gt;layer&lt;/em&gt;, türk dilli ədəbiyyatlarda &lt;em&gt;katman&lt;/em&gt;) adlanır. Şəbəkənin giriş dəyərləri birlikdə giriş qatı, çıxışı isə çıxış qatı adlanır. Bura qədər hələ ki, bildiyimiz terminlər və dəyərlərlə qarşılaşdıq. Ancaq bizə &lt;em&gt;gizli&lt;/em&gt; qalan bir xarakteristikanı hələ vurğulamadım; şəbəkələrdə giriş qatı və çıxış arasında olan hesablama mərhələləri &lt;em&gt;gizli qatlar&lt;/em&gt; (ing. hidden layers) adlanır. Bir neçə gizli qatı olan neyron şəbəkə isə &lt;strong&gt;çoxqatlı SNŞ&lt;/strong&gt; adlanır.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Əlavə məlumat:&lt;/strong&gt; SNŞ-də gizli qatın istifadə edilməsinin riyazi səbəbinə daha ətraflı girmək istəmirəm. Səbəbi bu mövzunun bizim danışdığımız konteksdən bir qədər uzaq olmasıdır. Ancaq bunu qeyd etmək istəyirəm ki, nisbətən sadə (1 və ya 2 gizli qatlı) şəbəkə belə istənilən sərhədlənmiş təyin oblastı olan silsiləvi funksiyanı təxmin edə bilər. Əgər riyazi analiz hissəsi sizə çox maraqlıdırsa, bu &lt;a href=&quot;https://towardsdatascience.com/representation-power-of-neural-networks-8e99a383586&quot;&gt;linkdəki&lt;/a&gt; yazı yaxşı giriş ola bilər.&lt;/p&gt;

&lt;p&gt;Aşağıda gördüyünüz neyron şəbəkə(şəkil EPFL universitetin &lt;a href=&quot;https://mlo.epfl.ch/page-157255-en-html/&quot;&gt;“Machine Learning”&lt;/a&gt; kursunun materiallarından götürülmüşdür.) hər biri &lt;em&gt;K&lt;/em&gt; ölçüdə olan &lt;em&gt;L&lt;/em&gt; ədəd gizli qat, &lt;em&gt;D&lt;/em&gt; ölçülü giriş qatdan və çıxış qatdan ibarətdir. Şəkildə də gördüyünüz kimi hər hesablama mərhələsinin girişi bir əvvəlki qatın çıxışıdır(və ya şəbəkənin girişidir). Geriyə doğru döngü olmadığından və hər addım irəliyə olduğundan belə modellər ingilis dilli ədəbiyyatlarda &lt;em&gt;feedforward&lt;/em&gt;, türk dilli ədəbiyyatlarda &lt;em&gt;ileri beslemeli&lt;/em&gt; şəbəkə olaraq qeyd edilir. &lt;s&gt;Azərbaycan dilində bununla bağlı xüsusi bir mənbə tapa bilmədiyimdən və türk dilində də tərcümə zamanı ingilis dilli terminin hərfi mənası işlədiyindən mən də onu *irəli qidalı* şəbəkə adlandırmağa qərar verdim.(qəribə səslənir, bilirəm, əgər sizin də bir təklifiniz varsa, bizə yazsanız sevinərik.)&lt;/s&gt;. Bu yazını paylaşdıqdan sonra bu mövzu ilə bağlı bir neçə rəy aldım, “irəli qıdalı” biraz qəribə səsləndiyindən və hər bir qat öz dəyərini növbəti qata giriş olaraq ötürdüyündən tərcüməni “irəliyə ötürmə”-yə dəyişməyə qərara aldım.&lt;/p&gt;

&lt;div class=&quot;center&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/DERINtelligence/web/master/images/neuralnetwork.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Deməli, bu irəliyə ötürməli şəbəkədə gizli qat &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;-dəki (hansı ki, &lt;script type=&quot;math/tex&quot;&gt;l = 1, \dots, L&lt;/script&gt;) hər bir neyron özündən bir əvvəlki qatdakı bütün neyronlarla əlaqəlidir. Gəlin, &lt;script type=&quot;math/tex&quot;&gt;l-1&lt;/script&gt; qatındakı &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; neyronundan &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; qatındakı &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; neyronuna olan əlaqənin ağırlığını &lt;script type=&quot;math/tex&quot;&gt;w_{i, j}^{(l)}&lt;/script&gt; ilə işarə edək. Bu halda &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; qatındakı &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; neyronunun dəyəri olan &lt;script type=&quot;math/tex&quot;&gt;x_j^{(l)}&lt;/script&gt;-ni aşağıdakı bərabərliklə ifadə edə bilərik.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
        x_j^{(l)} = \phi(\sum_i w_{i, j}^{(l)} x_i^{(l-1)} + b_j^{(l)})
\tag{3}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;Yuxarıda da qeyd etdiyim kimi mən bu yazıda aktivasiya funksiyası(&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;) üçün siqmoid funksiya istifadə edəcəm. Ona görə Bər. 3-də funksiyanı siqmoidlə əvəz edək, &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; qatındakı bütün neyronların dəyərini isə, vektor formasında bir daha yazaq.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
        z^{(l)} = w^{(l)} x^{(l-1)} + b^{(l)}
\tag{4}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
        x^{(l)} = \sigma(z^{(l)})
\tag{5}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;Neyron şəbəkənin yekun çıxış dəyərini almaq üçün isə şəkildən də gördüyümüz kimi son gizli qatın dəyərindən istifadə edirik.&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
        y = \sigma(z^{(L)})
\tag{6}\end{eqnarray}&lt;/p&gt;

&lt;p&gt;Beləliklə biz &lt;em&gt;irəliyə ötürmə&lt;/em&gt; alqoritmini bitirdik. Sadə dillə bir daha üzərindən keçsək, deyə bilərik ki, bu alqoritmlə giriş qatından başlayaraq hər dəfə bir sonrakı gizli qatı aktivləşdirib, sonda da çıxış dəyərini hesablıyırıq. İndi burada sual yaranır ki, bütün bu şəbəkənin əsas məqsədi olan təxmin etmə nə qədər dəqiqdir, yaratdığımız model təxmin etmək istədiyimiz dəyərə hansı dərəcədə yaxındır. Biz nəticədə, əlbəttə, təxminimizi minimum zərərlə etmək istəyirik. Bu nöqtədə bizə &lt;strong&gt;maşın öyrənmə&lt;/strong&gt; alqoritmləri lazım olacaq. Ancaq bu mövzuya bu yazıda girmək istəmirəm, ona görə də, gəlin burda yavaş-yavaş yekunlaşdıraq. Gələn yazıda artıq maşın öyrənmə alqoritmlərinə, itirmə funksiyalarına giriş edəcəyik. Növbəti yazılarda isə, onların irəliyə ötürməli şəbəkələrə necə uyğunlaşdıra biləcəyimizdən danışacağıq. Mən gələn yazıları yazdığım müddətdə siz xətti cəbr və riyazi analiz kimi mövzulara baxsanız əla olar, çünki, maşın öyrənmə alqoritmlərində riyazi düşüncə önəmlidir. Buraya qədər səbr edib oxuduğunuz üçün təşəkkürlər!&lt;/p&gt;</content><author><name>Mammad Hajili</name></author><summary type="html">Salamlar, aylar oldu bizdən yazı görmədiyiniz, bilirik. Deməli məsələ belə idi ki, bizim saytımız çökdü, problemlər oldu və biz daha sadə bir struktura keçməyə qərar verdik. Artıq yazılara Wordpress’dən yox, Jekyll-Now dizaynı olan sadə bir saytdan davam edəcəyik (bəlkə, ə’lər də xoşunuza gələr bu dəfə). Bu arada, neyronlar və siqmoid funksiya haqda olan iki yazı silinib, onları yenidən bərpa etməyə çalışacam.</summary></entry></feed>