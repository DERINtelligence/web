<!DOCTYPE html>
<html>
  <head>
    <title>Guess who's back'propagate. - Xəta funksiyası və stoxastik nöqtəvi meyilli azalma. – Derintelligence – A research project, supports AI in Azerbaijan</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="Salamlar, keçən yazıda süni neyron şəbəkənin arxitekturası haqqında danışdıq. Orada göstərdiyim “irəli qidalama” tərcüməsi ilə bağlı bir neçə mesaj aldım və biraz qəribə tərcümə olduğunu nəzərə alaraq onu “irəliyə ötürmə” kimi dəyişməyin daha düzgün olduğunu düşünürəm. Bundan əvvəlki yazıda da uyğun dəyişiklikləri edəcəyəm.

" />
    <meta property="og:description" content="Salamlar, keçən yazıda süni neyron şəbəkənin arxitekturası haqqında danışdıq. Orada göstərdiyim “irəli qidalama” tərcüməsi ilə bağlı bir neçə mesaj aldım və biraz qəribə tərcümə olduğunu nəzərə alaraq onu “irəliyə ötürmə” kimi dəyişməyin daha düzgün olduğunu düşünürəm. Bundan əvvəlki yazıda da uyğun dəyişiklikləri edəcəyəm.

" />
    
    <meta name="author" content="Derintelligence" />

    
    <meta property="og:title" content="Guess who's back'propagate. - Xəta funksiyası və stoxastik nöqtəvi meyilli azalma." />
    <meta property="twitter:title" content="Guess who's back'propagate. - Xəta funksiyası və stoxastik nöqtəvi meyilli azalma." />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="shortcut icon" type="image/png" href="/favicon.png">
    <link rel="alternate" type="application/rss+xml" title="Derintelligence - A research project, supports AI in Azerbaijan" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="http://127.0.0.1:4000/images/logodone.png" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">Derintelligence</a></h1>
            <p class="site-description">A research project, supports AI in Azerbaijan</p>
          </div>

          <nav>
            <a href="/">Blog</a>
            <a href="/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Guess who's back'propagate. - Xəta funksiyası və stoxastik nöqtəvi meyilli azalma.</h1>

  <div class="entry">
    <p>Salamlar, keçən <a href="http://derintelligence.az/suni_neyron_shebekeler_ireli_qidalama/">yazıda</a> süni neyron şəbəkənin arxitekturası haqqında danışdıq. Orada göstərdiyim “irəli qidalama” tərcüməsi ilə bağlı bir neçə mesaj aldım və biraz qəribə tərcümə olduğunu nəzərə alaraq onu “irəliyə ötürmə” kimi dəyişməyin daha düzgün olduğunu düşünürəm. Bundan əvvəlki yazıda da uyğun dəyişiklikləri edəcəyəm.</p>

<p>Bu dəfəki yazımızda maşın öyrənmənin ümumi izahından, istifadə olunan bərabərliklərdən və alqoritmlərdən - xəta funksiyasından və stoxastik nöqtəvi meyilli azalma haqqında danışacam. Bu yazını yazarkən, oxuyucumun ən azı maşın öyrənmənin nə olduğunu bildiyini ümid edirəm. Hazır olun, bu yazıda bol-bol riyazi ifadələr görəcəksiniz. Kəmərləri bağlayın, başlayırıq.</p>

<p><img src="https://raw.githubusercontent.com/DERINtelligence/web/master/images/paris_blog.jpg" style="width:100%" /></p>

<p>Əslində bu yazını yazmaq bir qədər çətindir, çünki, maşın öyrənmə alqoritmlərini tam izah etmək üçün 1 yox, 5 yazı bəs etməz, böyük ehtimalla. Ona görə də mən bu yazıda yalnızca bizim SNŞ-dəki hesablamalar üçün lazım olan önəmli nüanslara toxunacam. Belə ki, maşın öyrənmə alqoritmləri(biz burada öyrənməni müşahidəli(ing. supervised) olaraq hesab edirik) verilənlər toplusu <script type="math/tex">X={X_1, X_2, \cdots, X_N}</script> və ona uyğun cavablar <script type="math/tex">Y={Y_1, Y_2, \cdots, Y_N}</script> əsasında qeyri-xətti və ya xətti bir qanunauyğunluq taparaq verilənlər toplusu ilə eyni quruluşda olan yeni verilənə uyğun cavabı təxmin etmək üçün istifadə edilir. Bu iki ardıcıl prosesə “öyrən və təxmin et” (ing. learn and predict) də deyə bilərik. Ancaq bu prosesdə, məncə bir nüans əskikdi, sizcə, hansı?</p>

<p><script type="math/tex">N</script> ölçülü bu topluda ən optimistik halda biz hər bir elementin yalnızca uyğun cavab dəyəri ilə uyğunluğunu görə bilərik. Ancaq toplunun hər verilənin uyğunluğu individual olaraq bizim məsələdə bir məna kəsb etmir, çünki ixtiyari iki verilən - <script type="math/tex">X_i</script> və <script type="math/tex">X_j</script> fərqli-fərqli funksiyanın təyin oblastı ola bilər. Bizə ümumi qanunauyğunluğu tapmaq üçün bütün bu funksiyaları ehtiva edəcək ümumi funksiyanı - riyazi modeli tapmaq gərəklidir. Gəlin, bu funksiyanı <script type="math/tex">f(X)</script> ilə işarə edək. Ən sadə halda bu funksiya arqumentləri girişə uyğun ağırlıq vektoru və sürüşmə əmsalı olan xətti funksiya ola bilər. Biz də hesablamalar zamanı izahı daha asan olsun deyə bu funksiyadan istifadə edəcəyik.</p>

<p>\begin{eqnarray} 
		f(X_i) = w \cdot X_i + b
\tag{1}\end{eqnarray}</p>

<p>Yuxarıda qeyd etdiyim verilənlər toplusunda gördüyünüz kimi hər bir element <script type="math/tex">X_i</script>-in ona uyğun cavabı <script type="math/tex">Y_i</script> var. Deməli biz ümumi funksiyamızın təxmininin dəqiqliyini funksiyanın dəyərinin və verilən elementə uyğun cavabın bir-birinə nə qədər yaxın olduğu ilə müqayisə edə bilərik. Bu yaxınlığı/uzaqlığı hesablamaq üçün <strong>xəta funksiyasından</strong>(ing. loss/cost function) istifadə edirik. Bu məsələmizdə xəta funksiyası kimi kvadratik xəta funksiyasında istifadə edəcəyik. <script type="math/tex">f(X)</script> funksiyasının ümumi xətası bütün elementlərə uyğun xətaların cəmi olduğundan kvadratik funksiya cəm zamanı mənfi və müsbət xətaların bir-birini ixtisar etməsinin qarşısını alır. Gəlin <script type="math/tex">f(X)</script> funksiyasının xəta funksiyasını <script type="math/tex">L(X)</script> ilə işarə edək. Cəmi normallaşdırma üçün xətalar cəmini nöqtələrin sayına bölürük.</p>

<p>\begin{eqnarray} 
		L(X_i) = \frac{1}{2}{(Y_i - f(X_i))} ^ 2
\tag{2}\end{eqnarray}</p>

<p>\begin{eqnarray} 
		L(X) = \frac{1}{N}\sum_i L(X_i)
\tag{3}\end{eqnarray}</p>

<p><strong>Qeyd:</strong> Kvadratik funksiyanı <script type="math/tex">\frac{1}{2}</script> əmsalı ilə vurmağımızın səbəbi sonrakı hesablamalar zamanı törəmə aldığımızda qalıq əmsalsız nəticə əldə etməkdir. Bu ümumi qəbul edilmiş bir qaydadır, ancaq, təbii ki, bütün hesablamaları bu əmsalı nəzərə almadan da etsək, yenə də eyni nəticəyə gələcəyik.</p>

<p>Xəta funksiyası maşın öyrənmə alqoritmlərinin təməlini təşkil edir, belə ki, yuxarıda qeyd etdiyimiz əskik nüansı onu istifadə edərək tamamlaya bilərik. Bər. 1-də gördüyümüz kimi funksiyanın dəyəri ağırlıq və sürüşmə əmsalından asılıdır və Bər. 2-də gördüyümüz kimi xətanın dəyəri <script type="math/tex">f(X)</script>-in dəyərindən asılıdır. Yəni, bilavasitə, modelin xətası <script type="math/tex">f(X)</script> funksiyasının arqumentləri olan ağırlıq <script type="math/tex">w</script> və sürüşmə <script type="math/tex">b</script> dəyərlərindən asılıdır. Bizim məqsədimiz isə, ən dəqiq, yəni verilən cavaba ən yaxın dəyərlər təxmin etməkdir, başqa bir sözlə desək xətanı azaltmaqdır. Deməli, biz funksiyanın arqumentlərini dəyişməklə ən az xətalı modeli əldə edə bilərik. Yəni, ixtiyari təyin olunmuş <script type="math/tex">w^{(0)}</script> və <script type="math/tex">b^{(0)}</script>-dən başlayaraq, xəta azalana qədər onları <em>müəyyən üsulla</em> dəyişsək modelimiz öyrənmə prosesini bitirəcək. Ona görə də, mən təklif edirəm ki, bu iterativ məntiqi də nəzərə alaraq, gəlin, “öyrən və təxmin et” prosesini “öyrədilənə qədər davam et və təxmin et” olaraq dəyişək və əskik nüansı aradan qaldıraq.</p>

<p>Yuxarıdakı abzasda sondan ikinci cümlədə fikir versəniz “müəyyən üsulla” hissəsini xüsusi işarələdim. İndi bizi maraqlandıran isə bizə ən dəqiq modeli verəcək ən optimal <script type="math/tex">w</script> və <script type="math/tex">b</script>-ni hər iterasiyada hansı üsulu istifadə edərək dəyişəcəyimizi müəyyən etməkdir. Maşın öyrənmədə, təbii ki, bir-birindən fərqli müxtəlif üsullar təklif olunub, ancaq mən sizə SNŞ-lərin döyünən ürəyi olan <strong>stoxastik nöqtəvi meyilli azalmanı(SNMA)</strong>(ing. stochastic gradient descent(SGD)) göstərəcəm. Bu arada, bu mövzu hələ də üzərində kifayət qədər elmi araşdırma gedən mövzudur və dərin riyazi izahı vaxt alandır, ona görə də mən riyazi analizin axtarışını sizin ixtiyarınıza buraxıram. Bununla belə, bizə lazım olacaq riyazi izahı verəcəm, təbii ki.</p>

<div class="center">
<img src="https://raw.githubusercontent.com/DERINtelligence/web/master/images/sgd.png" />
</div>

<p>Belə ki, <strong>nöqtəvi meyil(ing. gradient)</strong> funksiyanın toxunanının müəyyən nöqtədəki meylidir. Nöqtəvi meyil, xəta funksiyasının artış istiqamətinin əksinə olan vektor olaraq da ifadə edilir. Ancaq biz burda qarışıqlıq olmasın deyə onun ədədi dəyəri ilə ifadə edəcəyik. Belə ki, ədədi dəyər ilə ifadə edildiyində nöqtəvi meyil, sadəcə xəta funksiyasının müəyyən nöqtədəki differensialının əksinə bərabər olur. <script type="math/tex">L</script> funksiyası <script type="math/tex">w</script> və <script type="math/tex">b</script>-dən asılı funksiya olduğundan, iki nöqtəvi meyildən istifadə edəcəyik. Belə ki, hər hansı bir arqumentin <script type="math/tex">t+1</script> anındakı dəyərini tapmaq üçün onun <script type="math/tex">t</script> anındakı dəyəri ilə həmin andakı nöqtəvi meylini toplayacağıq.</p>

<p>\begin{eqnarray} 
		w^{(t+1)} = w^{(t)} - \beta_1\frac{\partial L_n}{\partial w^{(t)}}
\tag{4}\end{eqnarray}</p>

<p>\begin{eqnarray} 
		b^{(t+1)} = b^{(t)} - \beta_2\frac{\partial L_n}{\partial b^{(t)}}
\tag{5}\end{eqnarray}</p>

<p>Burada <script type="math/tex">\beta_1</script> və <script type="math/tex">\beta_2</script> xəta dəyərinin çox böyük qiymətlərində <script type="math/tex">w</script> və <script type="math/tex">b</script>-nin kəskin şəkildə azalmasının qarşısını alaraq modelin normallaşdırılması üçün istifadə olunan əmsallardır. <script type="math/tex">L_n</script> isə verilənlər toplusundan ixtiyari seçilmiş <script type="math/tex">n</script> elementinin xəta dəyəridir. Bu element hər iterasiyada ixtiyari olaraq seçildiyindən bu alqoritmi <strong>stoxastik</strong> nöqtəvi meyilli azalma adlandırırıq.</p>

<p><strong>Əlavə tapşırıq:</strong> Bər. 2-də <script type="math/tex">L</script> funksiyasını görürük. Bu bərabərlik əsasında törəmələri həll edərək Bər 4. və Bər 5.-i yenidən göstərin.</p>

<p>Bir daha xatırlatmaq istəyirəm ki, <script type="math/tex">f(X)</script> funksiyasını izahı sadə olduğundan istifadə etdim, növbəti yazıda indi öyrəndiyimiz alqoritmlərin SNŞ-lərə tətbiqi zamanı <script type="math/tex">f(X)</script> yox, gizli qatların aktivasiyası və yekun çıxış qatının dəyərindən istifadə edəcəyik. Açığı, burada xəta funksiyası və SNMA-nı yaxşı anlasanız, növbəti yazı sizin üçün yalnızca mürəkkəb funksiyaların törəməsinin <strong>zəncir qaydası</strong> ilə həllini başa düşməkdən ibarət olacaq. Bəli, gələn yazıda seriyanın adını daşıyan geriyə yayılma(backpropagation) alqoritmindən danışacağıq.</p>

<p>Buraya qədər səbr edib oxuduğunuz üçün təşəkkürlər!</p>

  </div>

  <div class="date">
      February 18, 2019, Mammad Hajili
  </div>

  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'true';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:derintelligence@gmail.com"><i class="svg-icon email"></i></a>
<a href="https://www.facebook.com/derintelligence"><i class="svg-icon facebook"></i></a>

<a href="https://github.com/derintelligence"><i class="svg-icon github"></i></a>




<a href="https://www.twitter.com/derintelligence"><i class="svg-icon twitter"></i></a>



        </footer>
      </div>
    </div>

    

  </body>
</html>
